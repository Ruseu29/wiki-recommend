import streamlit as st
import requests
from datetime import date
import time

# ----------------------------
# ãƒšãƒ¼ã‚¸è¨­å®š
# ----------------------------
st.set_page_config(
    page_title="ä»Šæ—¥ã®Wikipediaãƒ©ãƒ³ãƒ€ãƒ è¨˜äº‹",
    layout="wide"
)

st.title("ğŸ“š ä»Šæ—¥ã®Wikipediaãƒ©ãƒ³ãƒ€ãƒ è¨˜äº‹")
st.caption("Wikipediaæ—¥æœ¬èªç‰ˆã‹ã‚‰ã€ä»Šæ—¥å‡ºä¼šã†5ã¤ã®è¨˜äº‹")

# ----------------------------
# 1æ—¥1å›åˆ¶é™ï¼ˆç°¡æ˜“ï¼‰
# ----------------------------
today = str(date.today())

if "last_fetch_date" not in st.session_state:
    st.session_state.last_fetch_date = None

if "articles" not in st.session_state:
    st.session_state.articles = []

can_fetch = st.session_state.last_fetch_date != today

# ----------------------------
# Wikipedia API
# ----------------------------
API_URL = "https://ja.wikipedia.org/api/rest_v1/page/random/summary"
HEADERS = {
    "User-Agent": "KU-WebProgramming-Student/1.0 (learning project)"
}
def fetch_random_articles(n=5):
    articles = []

    try:
        for _ in range(n):
            r = requests.get(API_URL,headers=HEADERS, timeout=10)
            r.raise_for_status()      # HTTPã‚¨ãƒ©ãƒ¼æ¤œçŸ¥
            data = r.json()           # JSONã§ãªã‘ã‚Œã°ä¾‹å¤–

            articles.append({
                "title": data.get("title"),
                "description": data.get("description", ""),
                "extract": data.get("extract", ""),
                "url": data.get("content_urls", {})
                            .get("desktop", {})
                            .get("page", ""),
                "image": data.get("thumbnail", {})
                            .get("source", None)
            })

    except Exception as e:
        # é€”ä¸­ã§å¤±æ•—ã—ãŸã‚‰ã€Œå…¨ä½“å¤±æ•—ã€ã¨ã™ã‚‹
        with open("out", "a", encoding="utf-8") as f:
            f.write(f"{time.time()} ERROR: {e}\n")
        return None

    return articles

# ----------------------------
# ãƒœã‚¿ãƒ³
# ----------------------------
if st.button("ğŸ”„ ä»Šæ—¥ã®è¨˜äº‹ã‚’å–å¾—", disabled=not can_fetch):
    result = fetch_random_articles()

    if result is None:
        st.error("è¨˜äº‹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ™‚é–“ã‚’ãŠã„ã¦ã€ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
    else:
        st.session_state.articles = result
        st.session_state.last_fetch_date = today
        st.rerun()

# ----------------------------
# è¨˜äº‹è¡¨ç¤º
# ----------------------------
for article in st.session_state.articles:
    st.markdown("---")
    col_left, col_right = st.columns([3, 1])

    with col_left:
        # å¤§ã‚¿ã‚¤ãƒˆãƒ«
        st.markdown(
            f"## {article['title']}"
        )

        # å°è¦‹å‡ºã—ã‚µã‚¤ã‚ºã®æ¦‚è¦
        if article["description"]:
            st.markdown(
                f"### {article['description']}"
            )

        # æœ¬æ–‡è¦ç´„
        st.write(article["extract"])

        # ãƒªãƒ³ã‚¯
        st.markdown(
            f"[Wikipediaã§èª­ã‚€]({article['url']})"
        )

    with col_right:
        if article["image"]:
            st.image(article["image"], use_container_width=True)
