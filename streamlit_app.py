import streamlit as st
import requests
from datetime import date
import time
from supabase import create_client
import random

# ----------------------------
# Supabase è¨­å®š
# ----------------------------
SUPABASE_URL = "https://rglluudszoxeuxciupbx.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InJnbGx1dWRzem94ZXV4Y2l1cGJ4Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NjkxMzYzMTIsImV4cCI6MjA4NDcxMjMxMn0.ym3Aq9n8YMhRsFIRcsDKmyZCIz9fPNLdwqcKGUp_uhY"
SUPABASE_TABLE = "save_time_key_value"

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# ----------------------------
# ãƒšãƒ¼ã‚¸è¨­å®š
# ----------------------------
st.set_page_config(
    page_title="ä»Šæ—¥ã®Wikipediaãƒ©ãƒ³ãƒ€ãƒ è¨˜äº‹",
    layout="wide"
)

st.title("ğŸ“š ä»Šæ—¥ã®Wikipediaãƒ©ãƒ³ãƒ€ãƒ è¨˜äº‹")
st.caption("Wikipediaæ—¥æœ¬èªç‰ˆã‹ã‚‰ã€ä»Šæ—¥å‡ºä¼šã†è¨˜äº‹")
tag_choices = {
    # ç”Ÿãç‰©ãƒ»è‡ªç„¶ï¼ˆè„³æ­»ã§è¦‹ã‚Œã‚‹ç³»ï¼‰
    "å“ºä¹³é¡": "Category:å“ºä¹³é¡",
    "é³¥é¡": "Category:é³¥é¡",
    "æç«œ": "Category:æç«œ",
    "æ·±æµ·ç”Ÿç‰©": "Category:æ·±æµ·ç”Ÿç‰©",

    # äººç‰©ï¼ˆçŸ¥çš„ãƒ»å®‰å®šï¼‰
    "æ•°å­¦è€…": "Category:æ•°å­¦è€…",
    "æ—¥æœ¬ã®æ•°å­¦è€…": "Category:æ—¥æœ¬ã®æ•°å­¦è€…",
    "ç‰©ç†å­¦è€…": "Category:ç‰©ç†å­¦è€…",
    "å“²å­¦è€…": "Category:å“²å­¦è€…",
    "è¨ˆç®—æ©Ÿç§‘å­¦è€…": "Category:è¨ˆç®—æ©Ÿç§‘å­¦è€…",

    # å»ºé€ ç‰©ãƒ»å ´æ‰€ï¼ˆç”»åƒæ˜ ãˆï¼‰
    "ä¸–ç•Œéºç”£": "Category:ä¸–ç•Œéºç”£",
    "åŸ": "Category:åŸ",
    "æ©‹": "Category:æ©‹",

    # é›‘å­¦ãƒ»è»½ã‚
    "ç¥æ—¥": "Category:ç¥æ—¥",
    "æ—¥æœ¬ã®ç¥æ—¥": "Category:æ—¥æœ¬ã®ç¥æ—¥",
    "æ–™ç†": "Category:æ–™ç†",
    "é£Ÿæ": "Category:é£Ÿæ"
}

# ----------------------------
# 1æ—¥1å›åˆ¶é™ï¼ˆç°¡æ˜“ï¼‰
# ----------------------------
today = str(date.today())

# ----------------------------
# ã‚«ãƒ†ã‚´ãƒªåˆ¥ 1æ—¥1å›åˆ¶é™
# ----------------------------
if "last_fetch_by_category" not in st.session_state:
    # ä¾‹: {"å“ºä¹³é¡": "2026-01-30", "æ•°å­¦è€…": None}
    st.session_state.last_fetch_by_category = {}
if "articles_by_category" not in st.session_state:
    # ä¾‹: {"å“ºä¹³é¡": [...], "æ•°å­¦è€…": [...]}
    st.session_state.articles_by_category = {}
if "can_fetch" not in st.session_state:
    st.session_state.can_fetch = {category:{'date':today,'boolian':True} for category in tag_choices.values()}

for category in tag_choices.values():
    last = st.session_state.can_fetch.get(category)
    st.session_state.can_fetch[category] = (last != today)

# ----------------------------
# Wikipedia API
# ----------------------------
API_URL = "https://ja.wikipedia.org/w/api.php"
SUMMARY_URL = "https://ja.wikipedia.org/api/rest_v1/page/summary/"
HEADERS = {
    "User-Agent": "KU-WebProgramming-Student/1.0 (learning project)"
}

def fetch_random_articles(param,n=5):
    if param == 'random':
        param = random.choice(list(tag_choices.keys()))
    param = dict(tag_choices).get(param,param)
    can_fetch = st.session_state.can_fetch.get(param)
    if not can_fetch:
        print('already exist.')
        return st.session_state.articles_by_category.get(param, [])
    articles = []
    request = {
    "action": "query",            # æƒ…å ±å–å¾—ãƒ¢ãƒ¼ãƒ‰
    "list": "categorymembers",    # ã‚«ãƒ†ã‚´ãƒªã«å±ã™ã‚‹ãƒšãƒ¼ã‚¸ä¸€è¦§
    # "cmnamespace": 0,             # é€šå¸¸è¨˜äº‹ã®ã¿
    "cmtitle": param, # å¯¾è±¡ã‚«ãƒ†ã‚´ãƒªï¼ˆã“ã“ã‚’å·®ã—æ›¿ãˆã‚‹ï¼‰
    "cmlimit": 20,                # å–å¾—ä»¶æ•°ï¼ˆè¨˜äº‹æ•°ï¼‰
    "format": "json",             # JSONã§è¿”ã™
    }
    try:
        r = requests.get(API_URL, headers=HEADERS ,params=request, timeout=10)
        data = r.json()
        titles = [p["title"] for p in data["query"]["categorymembers"]]
        picked = random.sample(titles, k=min(n, len(titles)))
        print(picked)
        for title in picked:
            r = requests.get(SUMMARY_URL + title, headers=HEADERS, timeout=10)
            r.raise_for_status()
            data = r.json()

            articles.append({
                "title": data.get("title"),
                "description": data.get("description", ""),
                "extract": data.get("extract", ""),
                "url": data.get("content_urls", {})
                            .get("desktop", {})
                            .get("page", ""),
                "image": data.get("thumbnail", {})
                            .get("source", None),
            })
    except Exception as e:
        with open("out", "a", encoding="utf-8") as f:
            f.write(f"{time.time()} ERROR: {e}\n")
        return None

    print('articles ->',articles)
    return articles,param

# ----------------------------
# DB ã«ã‚¯ãƒªãƒƒã‚¯ã‚’è¨˜éŒ²
# ----------------------------
def send_click_to_db(url, title):
    # 1) ä»Šã® click ã‚’å–å¾—
    res = (
        supabase
        .table(SUPABASE_TABLE)
        .select("click")
        .eq("url", url)
        .execute()
    )

    if res.data:
        new_click = res.data[0]["click"] + 1
    else:
        new_click = 1

    # 2) upsert ã§ä¿å­˜
    supabase.table(SUPABASE_TABLE).upsert({
        "url": url,
        "title": title,
        "click": new_click
    }).execute()


# ----------------------------
# è¨˜äº‹å–å¾—ãƒœã‚¿ãƒ³
# ----------------------------
st.session_state.articles = []
param = st.session_state.get('selected_category',"random")
col1, col2 = st.columns([2, 1])
with col1:
    if st.button("ğŸ”„ ä»Šæ—¥ã®è¨˜äº‹ã‚’å–å¾—"):
        result,param = fetch_random_articles(param)

        if result is None:
            st.error("è¨˜äº‹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        else:
            st.session_state.articles_by_category[param] = result
            st.session_state.last_fetch_by_category[param] = today
            st.session_state.selected_category = param  # â˜… è¿½åŠ 
            st.rerun()
with col2:
    param = st.selectbox(
        "ã‚«ãƒ†ã‚´ãƒª",
        options=['random'] + list(tag_choices.keys()),
        index=0,
        key="selected_category"
    )

# ----------------------------
# è¨˜äº‹è¡¨ç¤º
# ----------------------------
if "selected_category" in st.session_state:
    st.write(f"é¸æŠã‚«ãƒ†ã‚´ãƒª: {st.session_state.selected_category[9:]}")
for article in st.session_state.articles_by_category.get(param,[]):
    st.markdown("---")
    col_left, col_right = st.columns([3, 1])

    with col_left:
        st.markdown(f"## {article['title']}")

        if article["description"]:
            st.markdown(f"### {article['description']}")

        st.write(article["extract"])

        if st.button("Wikipediaã§èª­ã‚€", key=f"read_{article['url']}"):
            send_click_to_db(article["url"], article["title"])
            st.success("ğŸ“Š ã‚¯ãƒªãƒƒã‚¯ã‚’è¨˜éŒ²ã—ã¾ã—ãŸã€‚Wikipediaã¸ç§»å‹•ã—ã¾ã™â€¦")

            st.markdown(
                f"""
                <meta http-equiv="refresh" content="0; url={article['url']}">
                """,
                unsafe_allow_html=True
            )
            st.stop()

    with col_right:
        if article["image"]:
            st.image(article["image"], width="stretch")

# ----------------------------
# äººæ°—è¨˜äº‹ ä¸Šä½3ä»¶
# ----------------------------
st.markdown("---")
st.subheader("ğŸ”¥ äººæ°—ã®è¨˜äº‹")

top = (
    supabase
    .table(SUPABASE_TABLE)
    .select("title,url,click,last_clicked_at")
    .order("click", desc=True)
    .limit(3)
    .execute()
)

for row in top.data:
    col_left, col_right = st.columns([4, 1])

    with col_left:
        st.write(row["title"])

    with col_right:
        if st.button("èª­ã‚€", key=f"popular_{row['url']}"):
            send_click_to_db(row["url"], row["title"])
            st.success("ğŸ“Š ã‚¯ãƒªãƒƒã‚¯ã‚’è¨˜éŒ²ã—ã¾ã—ãŸã€‚Wikipediaã¸ç§»å‹•ã—ã¾ã™â€¦")

            st.markdown(
                f"""
                <meta http-equiv="refresh" content="0; url={row['url']}">
                """,
                unsafe_allow_html=True
            )
            st.stop()
